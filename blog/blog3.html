<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <title>Graphs, But Smarter: The Rise of Graph Neural Networks | Amir Bazzi</title>

  <!-- Tailwind (CDN) -->
  <script src="https://cdn.tailwindcss.com"></script>
  <script>
    tailwind.config = {
      theme: {
        extend: {
          colors: { brand: { DEFAULT: '#4f46e5' } },  // indigo-600
          fontFamily: {
            sans: ['Inter','ui-sans-serif','system-ui','Segoe UI','Roboto','Helvetica','Arial','sans-serif'],
            serif: ['"Source Serif 4"','Georgia','Cambria','Times New Roman','Times','serif'],
          }
        }
      }
    }
  </script>

  <!-- Fonts -->
  <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
  <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600;700&family=Source+Serif+4:opsz,wght@8..60,400;8..60,600;8..60,700&display=swap" rel="stylesheet" />

  <!-- MathJax for LaTeX -->
  <script async id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js"></script>

  <style>
    html { scroll-behavior: smooth; }
    body { -webkit-font-smoothing: antialiased; -moz-osx-font-smoothing: grayscale; }

    /* Article typography & spacing (bigger + wider) */
    .article { font-family: "Source Serif 4", Georgia, Cambria, "Times New Roman", Times, serif; max-width: 84ch; }
    .article h2 { margin-top: 2.4rem; margin-bottom: 1rem; letter-spacing:-0.01em; font-weight:700; font-size: 1.9rem; }
    .article p  { margin: 1.25em 0; line-height: 2.0; font-size: 1.25rem; }
    .article li { line-height: 2.0; margin: .55em 0; font-size: 1.25rem; }
    .article ul, .article ol { margin: 1.15em 0 1.4em; padding-left: 1.25rem; }
    .article a { text-decoration: underline; text-underline-offset: 3px; text-decoration-thickness: 1.5px; }
    .article a:hover { text-decoration-color: #4f46e5; }

    .figure { margin: 1.35rem 0 1.9rem; }
    .figure img { border-radius: .8rem; }
    .caption { text-align:center; font-size: .95rem; color: #6b7280; margin-top: .55rem; }

    /* Optional: nicer quotes */
    .article blockquote { border-left: 4px solid #e5e7eb; padding-left: 1rem; color:#4b5563; font-style: normal; }
  </style>
</head>
<body class="bg-gray-50 text-gray-800 font-sans">

  <!-- Top nav -->
  <header class="fixed top-0 inset-x-0 z-50 bg-white/90 backdrop-blur shadow-sm">
    <div class="max-w-7xl mx-auto px-4 sm:px-6 lg:px-8">
      <div class="flex justify-between items-center h-16">
        <a href="../index.html" class="text-[1.15rem] font-semibold tracking-tight text-brand">Amir&nbsp;Bazzi</a>
        <nav class="hidden md:flex items-center gap-8 text-[0.95rem]">
          <a href="../about.html" class="text-gray-600 hover:text-brand">About</a>
          <a href="../work.html"  class="text-gray-600 hover:text-brand">Work</a>
          <a href="../blog.html"  class="text-gray-600 hover:text-brand">Blog</a>
          <a href="../contact.html" class="text-gray-600 hover:text-brand">Contact</a>
        </nav>
      </div>
    </div>
  </header>

  <!-- Subtle banner -->
  <section class="relative pt-20 pb-16 bg-center bg-cover" style="background-image:url('../assets/medium/hero_gnn.webp')">
    <div class="absolute inset-0 bg-black/30"></div>
    <div class="relative max-w-6xl mx-auto px-4"></div>
  </section>

  <!-- Clear title block (wider & readable) -->
  <section class="bg-white border-b">
    <div class="max-w-5xl mx-auto px-4 sm:px-6 lg:px-8 py-10">
      <h1 class="text-5xl sm:text-6xl font-semibold tracking-tight text-gray-900">
        Graphs, But Smarter: The Rise of Graph Neural Networks
      </h1>
      <p class="mt-3 text-sm text-gray-500">Published April&nbsp;24,&nbsp;2025</p>
    </div>
  </section>

  <!-- Article card: wider padding + subtle frame -->
  <article class="article mx-auto px-8 sm:px-10 lg:px-14 py-12 bg-white rounded-2xl shadow-sm ring-1 ring-gray-200">

    <h2>Introduction</h2>
    <p>The success of neural networks has often been traced back to their biological inspiration: the human brain. Just as our brains are composed of neurons that pass signals to one another, artificial neural networks replicate the idea by passing information through layers of interconnected nodes. This simple yet powerful concept led to one of the most important breakthroughs of the century.</p>
    <p>But while this neural analogy did great, most traditional networks are still limited in how they see the world — they assume that data comes in neat grids (like images) or ordered sequences (like text). The real world, however, is rarely that tidy. Think of a molecule made of atoms and bonds, a social network of people and friendships. Those can be represented by <em>graphs</em> — and they’re everywhere. The theory of graphs dates back to Euler’s solution to the bridges of Königsberg in 1736 and since then these mathematical structures have become widely used across computer science, biology and chemistry.</p>
    <p>In 2008 Scariely&nbsp;et&nbsp;al. introduced the concept of the <strong>Graph Neural Network</strong>. Unlike traditional neural networks, GNNs are chaotic in their nature: they allow us to build models that don’t just learn from individual data points but from the <em>connections</em> between them. That makes them powerful tools for simulating physical systems, understanding complex interactions, and reasoning about structures that other networks can’t easily handle. In this article we’ll explore the basics of GNNs — what they are, how they work, and why they matter.</p>

    <h2>What is a Graph, Really?</h2>
    <p>At its core, a graph is just a collection of nodes (also called vertices) and edges. We denote it as \(G=(V,E)\) where \(V\) is the set of nodes and \(E\subseteq V\times V\) is the set of edges. These edges represent the connections between pairs of nodes.</p>
    <p>Beyond their mathematical elegance, graphs are incredibly versatile. They naturally arise in domains where data is best understood as a system of interactions:</p>
    <ul>
      <li><strong>Social networks</strong> — nodes are individuals and edges represent social ties.</li>
      <li><strong>Molecular chemistry</strong> — nodes are atoms and edges are chemical bonds.</li>
    </ul>
    <p>Graphs can also be <em>attributed</em>, meaning that nodes and edges carry associated feature vectors. For instance, a node could have properties like age and interests in a social network or velocity and temperature in a physical simulation. Edges might encode the type or strength of a relationship, the distance between two nodes, or even a direction. These features, along with the graph’s topology, form the input for models like Graph Neural Networks.</p>
    <p>What distinguishes graph-structured data from other formats is its irregularity. Unlike images (regular 2D grids) or sequences (ordered 1D lists), graphs don’t impose a fixed structure. They can vary in size and the number of neighbors each node has. This irregularity is precisely why traditional neural network architectures fall short — they’re not designed to handle variable neighborhood sizes. GNNs address this by operating directly on graphs, leveraging their structure to perform computations that respect the relational nature of the data.</p>

    <h2>What Makes GNNs Different?</h2>
    <p>Traditional neural networks — such as CNNs or RNNs — operate on structured, regular domains like grids or sequences. Their success largely relies on built-in assumptions about data structure, known as <em>inductive biases</em>, that help models generalize beyond the training examples. For instance, convolutional networks embed the bias of <em>translation equivariance</em>, meaning they recognize patterns regardless of their exact position in an image, thanks to shared weights.</p>
    <p>But real-world data often defies these tidy assumptions. Data such as social networks, molecules, meshes or interacting particles comes as graphs: irregular structures consisting of nodes and edges with arbitrary connectivity. Traditional models struggle here because the critical relationships aren’t defined by fixed spatial or sequential order but by complex interconnections.</p>
    <p>Graph Neural Networks address this limitation by embedding a different type of inductive bias, suited explicitly for relational data. Central to this is <strong>permutation invariance</strong>: the idea that a graph’s identity doesn’t change if you rearrange the order of its nodes. Another important property is <strong>permutation equivariance</strong> for node-level outputs: if you permute the nodes of the input, the outputs should be permuted in the same way.</p>
    <p>This idea is rooted in graph isomorphism. If two graphs are structurally identical (even if their node labels differ), a good GNN should produce the same result. That’s why permutation invariance and equivariance are so important: the model must respect the structure, not the label order. As you explore different GNN architectures (e.g., GCN, GraphSAGE, GAT or GIN), always check whether they maintain these properties.</p>

    <div class="figure">
      <img src="../assets/medium/equivariance.png" alt="Equivariance demonstration" loading="lazy" />
      <div class="caption">Permutation equivariance: reordering inputs reorders outputs the same way.</div>
    </div>

    <h2>How to choose your GNN?</h2>
    <p>Next, what we’re looking to do is to choose the most expressive GNN architecture. But first we need to ask: how powerful are GNNs really? The <em>expressive power</em> of a GNN refers to its ability to distinguish different graph structures — a core challenge since graphs are inherently complex with no fixed node ordering or spatial locality.</p>
    <p>A classical test of expressiveness is <strong>graph isomorphism</strong>: can the GNN distinguish two non-isomorphic graphs? Message-passing GNNs, the most common class, operate by passing and aggregating messages between neighboring nodes. Each node updates its representation by aggregating messages from its neighbors, and different GNNs (e.g., GCN, GraphSAGE, GAT) differ in how they perform this aggregation. However, not all aggregation functions are equally powerful.</p>
    <p>We can model GNN neighborhood aggregation as a function over a <em>multiset</em> (a set where elements may repeat), since node neighborhoods can contain repeated features. An expressive GNN requires its aggregation function to be <strong>injective</strong>: different multisets must yield different outputs. If the function isn’t injective, two different neighborhoods could produce the same embedding.</p>

    <p>For instance, consider two multisets of node features:</p>
    <ul>
      <li>Multiset A: [a, a, b, b]</li>
      <li>Multiset B: [a, b]</li>
    </ul>

    <div class="figure">
      <img src="../assets/medium/mean_agg.png" alt="Mean pooling example" loading="lazy" />
      <div class="caption">Using mean pooling, two different multisets can collapse to the same value, making the aggregator non-injective.</div>
    </div>

    <div class="figure">
      <img src="../assets/medium/same_output.png" alt="Two neighborhoods with same output" loading="lazy" />
      <div class="caption">Two different neighborhoods producing the same output under a non-injective aggregation.</div>
    </div>

    <h2>Okay, then what is the best aggregator?</h2>
    <p>The key theoretical insight comes from Xu&nbsp;et&nbsp;al. (ICLR&nbsp;2019): a GNN can become maximally expressive if its aggregation function is <em>injective</em> over multisets. One such injective formulation is</p>
    <p>\[\mathrm{AGG}(\{h_u : u \in \mathcal{N}(v)\})=\phi\!\left(\sum_{u\in \mathcal{N}(v)} \psi(h_u)\right)\]</p>
    <p>The critical idea is that <strong>sum aggregation</strong> — unlike mean or max — retains information. This is the foundation of <strong>GIN</strong>, which sums transformed neighbor features and passes the result through an MLP, matching the expressive power of the 1-WL test.</p>

    <div class="figure">
      <img src="../assets/medium/injective_function.png" alt="Injective vs non-injective mapping" loading="lazy" />
      <div class="caption">Only injective mappings uniquely identify inputs.</div>
    </div>

    <p>However, GIN still has limitations (e.g., counting specific substructures). Two enhancements are common:</p>
    <ul>
      <li><strong>Structure-aware GNNs</strong> add features like cycle counts or centralities.</li>
      <li><strong>Position-aware GNNs</strong> embed nodes based on distances to anchor nodes/sets.</li>
    </ul>

    <div class="figure">
      <img src="../assets/medium/graph_subtrees.png" alt="Graph and rooted subtrees" loading="lazy" />
      <div class="caption">A graph and its rooted subtrees illustrating neighborhood structures.</div>
    </div>

    <div class="figure">
      <img src="../assets/medium/embedding_space.png" alt="Embedding space of subtrees" loading="lazy" />
      <div class="caption">Distinct subtrees map to distinct points in latent space.</div>
    </div>

    <h2>Wrapping up</h2>
    <p>Among message-passing GNNs, sum-based GINs are the most expressive for distinguishing neighborhood structures, but model choice still depends on task needs (scalability, edge importance, data regime). GATs can help when edge weighting matters; GCNs remain simple and efficient.</p>

    <h2>References</h2>
    <ul>
      <li><strong>Kipf &amp; Welling (2017)</strong> — <em>Semi-Supervised Classification with Graph Convolutional Networks</em>.</li>
      <li><strong>Hamilton et&nbsp;al. (2017)</strong> — <em>Inductive Representation Learning on Large Graphs</em>.</li>
      <li><strong>Veličković et&nbsp;al. (2018)</strong> — <em>Graph Attention Networks</em>.</li>
      <li><strong>Xu et&nbsp;al. (2019)</strong> — <em>How Powerful Are Graph Neural Networks?</em>.</li>
      <li><strong>You et&nbsp;al. (2021)</strong> — <em>Identity-Aware Graph Neural Networks</em>.</li>
      <li><strong>You et&nbsp;al. (2019)</strong> — <em>Position-Aware Graph Neural Networks</em>.</li>
      <li><strong>CS224W Lecture Notes</strong> — <em>Machine Learning with Graphs</em>.</li>
    </ul>

    <div class="mt-10 flex items-center gap-6">
      <a href="../blog.html" class="text-brand font-semibold hover:underline underline-offset-4">&larr; Back to Blog</a>
      <a href="https://medium.com/@bazziamir/graphs-but-smarter-the-rise-of-graph-neural-networks-ca42ff36ae74" target="_blank" rel="noopener" class="text-gray-500 hover:text-gray-700">Read on Medium &rarr;</a>
    </div>
  </article>

  <footer class="bg-white/80 border-t mt-10">
    <div class="max-w-5xl mx-auto px-4 sm:px-6 lg:px-8 py-8 text-center text-sm text-gray-500">
      © 2025 Amir Bazzi. All rights reserved.
    </div>
  </footer>
</body>
</html>
